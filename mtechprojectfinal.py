# -*- coding: utf-8 -*-
"""MTechProjectFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/185WScNP1bRW3I_GABHLQ57uJivpZvwOn
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install --upgrade tensorflow

!pip install keras

!pip install --upgrade keras

!pip show tensorflow

!pip show keras

import os
import librosa
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, MaxPool2D, GlobalAveragePooling2D, Dense, Dropout, AveragePooling2D, Flatten
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import GroupKFold


# Function to extract spectrograms and resize them
def extract_and_resize_spectrogram(audio_file, target_shape=(128, 128)):
    y, sr = librosa.load(audio_file, sr=22050)
    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)
    resized_spectrogram = np.resize(spectrogram, target_shape)
    return resized_spectrogram

# Path to the GTZAN dataset folders (each class label has its own folder)
gtzan_path = '/content/drive/MyDrive/Datasets/Music_Genre_Classification/genres'

# Initialize arrays
data_array = []
label_array = []
group_array = []

# Iterate through each class label folder
class_folders = sorted(os.listdir(gtzan_path))
for class_label, class_folder in enumerate(class_folders):
    class_folder_path = os.path.join(gtzan_path, class_folder)
    audio_files = os.listdir(class_folder_path)
    for audio_file in audio_files:
        audio_file_path = os.path.join(class_folder_path, audio_file)
        spectrogram = extract_and_resize_spectrogram(audio_file_path)
        data_array.append(spectrogram)
        label_array.append(class_label)
        group_array.append(class_label)

# Convert arrays to NumPy arrays
data_array = np.array(data_array)
label_array = np.array(label_array)
group_array = np.array(group_array)

# Define the 1D CNN model
def cnnmodel_tuned(input_shape):
    model = Sequential()
    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(GlobalAveragePooling2D())
    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes
    return model



# Initialize GroupKFold
gkf = GroupKFold(n_splits=3)


# Example usage of GroupKFold with CNN model training
accuracy = []
for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):
    train_features, train_labels = data_array[train_index], label_array[train_index]
    val_features, val_labels = data_array[val_index], label_array[val_index]

    # Reshape data for 1D CNN input
    train_features = np.expand_dims(train_features, axis=-1)
    val_features = np.expand_dims(val_features, axis=-1)

    # Create and compile the CNN model
    model = cnnmodel_tuned(input_shape=train_features.shape[1:])
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(train_features, train_labels, epochs=100, batch_size=32, validation_data=(val_features, val_labels))

    # Evaluate the model
    _, acc = model.evaluate(val_features, val_labels)
    accuracy.append(acc)

import os
import librosa
import numpy as np
from sklearn.model_selection import GroupKFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, MaxPool2D, GlobalAveragePooling2D, Dense, Dropout, AveragePooling2D, Flatten
from tensorflow.keras.optimizers import Adam

# Function to extract spectrograms and resize them
def extract_and_resize_spectrogram(audio_file, target_shape=(128, 128)):
    y, sr = librosa.load(audio_file, sr=22050)
    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)
    resized_spectrogram = np.resize(spectrogram, target_shape)
    return resized_spectrogram

# Path to the GTZAN dataset folders (each class label has its own folder)
gtzan_path = '/content/drive/MyDrive/Datasets/Music_Genre_Classification/genres'

# Initialize arrays
data_array = []
label_array = []
group_array = []

# Iterate through each class label folder
class_folders = sorted(os.listdir(gtzan_path))
for class_label, class_folder in enumerate(class_folders):
    class_folder_path = os.path.join(gtzan_path, class_folder)
    audio_files = os.listdir(class_folder_path)
    for audio_file in audio_files:
        audio_file_path = os.path.join(class_folder_path, audio_file)
        spectrogram = extract_and_resize_spectrogram(audio_file_path)
        data_array.append(spectrogram)
        label_array.append(class_label)
        group_array.append(class_label)

# Convert arrays to NumPy arrays
data_array = np.array(data_array)
label_array = np.array(label_array)
group_array = np.array(group_array)

# Define the CNN model
def cnnmodel_tuned(input_shape):
    model = Sequential()
    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(GlobalAveragePooling2D())
    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes
    return model

# Initialize GroupKFold
gkf = GroupKFold(n_splits=3)

# Example usage of GroupKFold with CNN model training
accuracy = []
for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):
    train_features, train_labels = data_array[train_index], label_array[train_index]
    val_features, val_labels = data_array[val_index], label_array[val_index]

    # Reshape data for CNN input
    train_features = np.expand_dims(train_features, axis=-1)
    val_features = np.expand_dims(val_features, axis=-1)

    # Create and compile the CNN model
    model = cnnmodel_tuned(input_shape=train_features.shape[1:])
    optimizer = Adam(learning_rate=0.0001)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(train_features, train_labels, epochs=100, batch_size=32, validation_data=(val_features, val_labels))

    # Evaluate the model
    _, acc = model.evaluate(val_features, val_labels)
    accuracy.append(acc)

# Print average accuracy
print("Average accuracy:", accuracy)

model.summary()

pip install --upgrade scipy

import os
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout
from tensorflow.keras.layers import BatchNormalization, LeakyReLU
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score


# Function to extract time-domain waveforms from audio files with fixed length
def extract_waveform(audio_file, label_index, duration=30, target_length=44100):
    y, sr = librosa.load(audio_file, duration=duration, sr=None)  # Load audio file (full duration, original sampling rate)
    # Trim or pad the waveform to the target length
    if len(y) < target_length:
        y = np.pad(y, (0, target_length - len(y)), mode='constant')
    elif len(y) > target_length:
        y = y[:target_length]
    return y, label_index

# Define paths to GTZAN dataset and class labels
gtzan_path = '/content/drive/MyDrive/Datasets/Music_Genre_Classification/genres'
class_labels = os.listdir(gtzan_path)

# Initialize lists to store waveforms and labels
X = []
y = []

# Iterate through each class label folder
for label_index, label in enumerate(class_labels):  # Use enumerate to get the index of each label
    class_folder = os.path.join(gtzan_path, label)
    for audio_file in os.listdir(class_folder):
        audio_file_path = os.path.join(class_folder, audio_file)
        waveform, label_index = extract_waveform(audio_file_path, label_index)
        X.append(waveform)
        y.append(label_index)  # Append the label index directly

# Convert lists to NumPy arrays
X = np.array(X)
y = np.array(y)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the input data for Conv1D layer
input_shape = (44100, 1)  # Shape is (timesteps, input_dim)

# Build the 1D CNN model
def cnnmodel_tuned(input_shape):
    model = Sequential()
    # Convolutional layers
    model.add(Conv1D(filters=32, kernel_size=3, strides=1, input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    model.add(MaxPooling1D(pool_size=2, strides=2))

    model.add(Conv1D(filters=64, kernel_size=3, strides=1))
    model.add(LeakyReLU())
    model.add(MaxPooling1D(pool_size=2, strides=2))
    model.add(Dropout(0.25))

    model.add(Conv1D(filters=128, kernel_size=3, strides=1))
    model.add(LeakyReLU())
    model.add(MaxPooling1D(pool_size=2, strides=2))
    model.add(Dropout(0.25))

    model.add(Conv1D(filters=256, kernel_size=3, strides=1))
    model.add(LeakyReLU())
    model.add(MaxPooling1D(pool_size=2, strides=2))

    # Global pooling and dense layers
    model.add(GlobalAveragePooling1D())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes

    return model


# Define the model
model = cnnmodel_tuned(input_shape)
# Compile the model
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on test data
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Predict on test data
y_pred = model.predict_classes(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Set Accuracy (from predictions): {test_accuracy:.4f}")

import os
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, Flatten
from tensorflow.keras.layers import BatchNormalization, LeakyReLU
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score

# Function to extract time-domain waveforms from audio files with fixed length
def extract_waveform(audio_file, label_index, duration=30, target_length=44100):
    y, sr = librosa.load(audio_file, duration=duration, sr=None)  # Load audio file (full duration, original sampling rate)
    # Trim or pad the waveform to the target length
    if len(y) < target_length:
        y = np.pad(y, (0, target_length - len(y)), mode='constant')
    elif len(y) > target_length:
        y = y[:target_length]
    return y, label_index

# Define paths to GTZAN dataset and class labels
gtzan_path = '/content/drive/MyDrive/Datasets/Music_Genre_Classification/genres'
class_labels = os.listdir(gtzan_path)

# Initialize lists to store waveforms and labels
X = []
y = []

# Iterate through each class label folder
for label_index, label in enumerate(class_labels):  # Use enumerate to get the index of each label
    class_folder = os.path.join(gtzan_path, label)
    for audio_file in os.listdir(class_folder):
        audio_file_path = os.path.join(class_folder, audio_file)
        waveform, label_index = extract_waveform(audio_file_path, label_index)
        X.append(waveform)
        y.append(label_index)  # Append the label index directly

# Convert lists to NumPy arrays
X = np.array(X)
y = np.array(y)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the input data for Conv2D layer
input_shape = (44100, 1, 1)  # Shape is (timesteps, input_dim, channels)

# Expand dimensions to match the input shape
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Build the 2D CNN model
def cnnmodel_tuned(input_shape):
    model = Sequential()
    # Convolutional layers
    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1)))
    model.add(LeakyReLU())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    # Flatten the output before dense layers
    model.add(Flatten())

    # Dense layers
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes

    return model

# Define the model
model = cnnmodel_tuned(input_shape)

# Compile the model
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on test data
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Predict on test data
y_pred = model.predict_classes(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Set Accuracy (from predictions): {test_accuracy:.4f}")